## Stat 154 Spring 2018

This repository holds the course materials for the Spring 2018 edition of 
__Statistics 154: Modern Statistical Prediction and Machine Learning__ at UC Berkeley.


- __Instructor:__ [Gaston Sanchez](http://gastonsanchez.com),  gasigiri [at] berkeley.edu
- __Class Time:__ MWF 11-12pm in [180 Tan](http://www.berkeley.edu/map?tan)
- __Session Dates:__ 01/17/18 - 05/04/18
- __Code #:__ 30887
- __Units:__ 4 (more info [here](http://classes.berkeley.edu/content/2018-spring-stat-154-001-lec-001))
- __Office Hours:__ MWF 3:00-4:00pm in 309 Evans (or by appointment)
- __Final:__ TBA
- __GSI:__ Omid Solari (OH TBD). 


| Lab | Date      | Room         | GSI             |
|-----|-----------|--------------|-----------------|
| 101 | M 12-2pm  | 334 Evans    | Omid Solari     |
| 102 | M 3-5pm   | 334 Evans    | Omid Solari     |



### Description

This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The [syllabus](syllabus) includes: linear regression, model assessment, model selection, regularization methods (pcr, plsr, ridge and lasso); logistic regression and  discriminant analysis; cross-validation and the bootstrap; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).

In this course, we will explore the predictive modeling lifecycle, including question formulation, data preprocessing, exploratory data analysis and visualization, model building, model assessment/validation, model selection, and decision-making.​ 

We will focus on quantitative critical thinking​ and key principles needed to carry out this cycle. 1) Foundational principles for building predictive models; 2) Intuitive explanations of many commonly used predictive modeling techniques for both classification and regression problems; 3) Principles and steps for validating a predictive model; and 4) write and use computer code to perform the necessary foundational work to build and validate predictive models.



### Contents

The course focuses on predictive models, and it covers the following 
topics (not necessarily in the displayed order):

- Process of predictive model building
- Data Preprocessing
- Regression Models
    + Linear models
    + Non-linear models (time permitting)
    + Tree-based methods
- Classification Models
    + Linear models
    + Non-linear models
    + Tree-based methods
    + Support Vector Machines (time permitting)
- Unsupervised methods like PCA and Clustering
- Data spending: splitting and resampling methods
- Model Evaluation
- Model Selection



### Prerequisites / Review

- Multivariate calculus or the equivalent, esp. partial derivatives; e.g. Math 53
- Linear algebra or the equivalent (matrices, vector spaces); e.g. Math 54
- Statistical inference or the equivalent; e.g. Stat 135
- Scripting experience in R required; e.g. Stat 133

This course will build on a lot of material from matrix algebra. In particular, you should be comfortable with notions such as vector spaces, inner products, norms, matrix products/transpose/rank/determinants/inverses, as well as matrix decompositions. 

You should also have some scripting experience---preferably in R---at the level of writing functions, conditionals (if-then-else structures), for loops, while loops, sampling, read in data sets, export results.

Last but not least, it is nice to know the basics of Rmd files, as well as some knowledge of LaTeX, especially some experience writing math symbols and equations.



### Textbooks

There is no official textbook for this course although we will use the following texts as supporting material:

- __An Introduction to Statistical Learning__ (ISL) by James, Witten, Hastie, and Tibshirani. Springer, 2013. It is freely available online in pdf format (courtesy of the authors) at [http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/).

- __The Elements of Statistical Learning__ by Hastie, Tibshirani and Friedman. Springer, 2009 (2nd Ed). This book is more mathematically-and-conceptually advanced than ISL. It is freely available online in pdf format (courtesy of the authors) at [https://statweb.stanford.edu/~tibs/ElemStatLearn/](https://statweb.stanford.edu/~tibs/ElemStatLearn/).

- __Applied Predictive Modeling__ by Max Kuhn and Kjell Johnson. Springer, 2013.

- __Data Mining and Statistics for Decision Making__ by Stephane Tuffery. Wiley 2011.



### Expectations

We expect that at the end of the course you:

- Have a basic, yet solid, understanding of the prediction modeling process/lifecycle.
- Be able to read a well-described algorithm, and write code to implement it 
computationally (in R).
- Know the pros and cons of each predictive technique.
- Be able to describe (to non-professionals) what a predictive technique is doing.



### Methods of Instruction

- We will be using a combination of materials such as slides, tutorials, 
reading assignments, and chalk-and-talk.
- The main computational tool will be the [computing and programming environment R](https://www.r-project.org/). 
- The main workbench will be the IDE [RStudio](https://www.rstudio.com/).
You will also use a terminal emulator to work with the command line.



### Other

- Please read the course [logistics and policies](syllabus/policies.md) for mode details
about the structure of the course, DO's and DONT's, etc.



-----

### License

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />Unless otherwise noticed, this work, by Gaston Sanchez, is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.

Author: [Gaston Sanchez](http://gastonsanchez.com)
